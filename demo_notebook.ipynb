{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T13:57:11.446240Z",
     "start_time": "2018-06-12T13:57:10.275012Z"
    }
   },
   "outputs": [],
   "source": [
    "from ovnlp.ft import weights as ws\n",
    "from ovnlp.ft import ftutils as ft\n",
    "from ovnlp.txtutils import cleantext as ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T13:57:12.826025Z",
     "start_time": "2018-06-12T13:57:12.818424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Pretrained weights are provided by FB for * cc *.\n",
      "Please use another name if you want to create your own model.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Weights already downloaded and extracted in /Users/fanch/ovnlp/fasttext/weights/cc/fr.\n"
     ]
    }
   ],
   "source": [
    "## Usage - Get pretrained weights\n",
    "# trainedOn parameter : \"cc\" stands for Common crawl, \"other\" or None is a custom model\n",
    "# iSavePath : where to save weights\n",
    "# if iSavePath=None, then homepath = HOME else homepath = iSavePath\n",
    "# Weights are saved in homepath+/ovnlp/fasttext/weights/ + iTrainedOn={cc or custom} + / + iLang={fr or en} + /\n",
    "ws1 = ws.WeightSource(iTrainedOn = \"cc\", iLang = \"fr\", iSavePath = None)\n",
    "\n",
    "# DL \"fr\" weigths, \"en\" may also be used (big file, long runtime)\n",
    "ws1.save_weights(iResave=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Utils - Load language specific objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T13:57:15.259765Z",
     "start_time": "2018-06-12T13:57:15.252032Z"
    }
   },
   "outputs": [],
   "source": [
    "# ovnlp.txtutils contains a cleantext module with a LangTools class to get objects language specific such as : stopwords, tokenizer, stemmer\n",
    "ltfr = ct.LangTools(\"fr\")\n",
    "stopwords = ltfr.get_stop_words(iCustomList = [\"``\",\"a\",\"l'\",\"s'\",\"d'\",\"n'\",\"c'\",\"les\",\"com\",\"_\",\"j'\"])\n",
    "stemmer = ltfr.get_stemmer()\n",
    "tokenizer = ltfr.get_tokenizer()\n",
    "# cleantext module also contains several utils functions : tokenize, text_file_to_sentence, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T13:57:51.427635Z",
     "start_time": "2018-06-12T13:57:17.651693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download sample text data (may take some time)\n",
    "import requests\n",
    "# DL Tome 1 to 4 of MonteCristo\n",
    "tomes=[] \n",
    "keys = [17989,17990,17991,17992]\n",
    "for i in keys:\n",
    "    r = requests.get(\"http://www.gutenberg.org/cache/epub/\"+str(i)+\"/pg\"+str(i)+\".txt\")\n",
    "    tomes.append(r.text)\n",
    "    \n",
    "end = \"End of the Project Gutenberg EBook\"\n",
    "start = \"www.ebooksgratuits.com\"\n",
    "\n",
    "# Keep Only text in french\n",
    "texteFull=tomes[0].split(start)[1].split(end)[0] + \".\" +\\\n",
    "tomes[1].split(start)[1].split(end)[0] + \".\" +\\\n",
    "tomes[2].split(start)[1].split(end)[0] + \".\" +\\\n",
    "tomes[3].split(start)[1].split(end)[0] + \".\"\n",
    "\n",
    "del tomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T13:58:04.731146Z",
     "start_time": "2018-06-12T13:57:51.429232Z"
    }
   },
   "outputs": [],
   "source": [
    "# use text utils funcs & stopwords / tokenizer for \"fr\" language\n",
    "# Use custom splitter here : split by punctuation to split sentences.\n",
    "sentences = ct.string_to_sentences(iString=texteFull, \n",
    "                                   iTokenizer=tokenizer, \n",
    "                                   iStopWords= stopwords,\n",
    "                                   iSplitter=\"\\.|\\?|!\",\n",
    "                                   iStemmer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T13:58:29.489234Z",
     "start_time": "2018-06-12T13:58:04.737819Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Custom model  : MonteCristo\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Custom model trained.\n",
      "Custom model saved in /Users/fanch/ovnlp/fasttext/weights/MonteCristo/fr/MonteCristo.fr.bin\n"
     ]
    }
   ],
   "source": [
    "# Instantiate WeightSource for custom model\n",
    "ws2 = ws.WeightSource(iTrainedOn = \"MonteCristo\", iLang=\"fr\", iSavePath = None)\n",
    "# Train custom model with 100 shaped embeddings : see https://radimrehurek.com/gensim/models/fasttext.html\n",
    "# for custom parameters : here is simple Word2Vec\n",
    "model  = ws.train_weights(sentences, iter=256, size=100, sg=1,word_ngrams=0)\n",
    "# Save model - overwrite\n",
    "ws2.save_weights(iTrainedModel=model, iResave=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T13:58:29.776638Z",
     "start_time": "2018-06-12T13:58:29.490852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Custom model  : MonteCristo\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading with FastText.load\n",
      "/Users/fanch/ovnlp/fasttext/weights/MonteCristo/fr/MonteCristo.fr.bin was loaded.\n"
     ]
    }
   ],
   "source": [
    "ws3 = ws.WeightSource(iTrainedOn = \"MonteCristo\", iLang=\"fr\",iSavePath = None)\n",
    "model = ws3.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of use with gensim API\n",
    "## see gensim doc for more \n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T13:58:29.801539Z",
     "start_time": "2018-06-12T13:58:29.778170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for word edmond :\n",
      "[ 0.23682924  0.78200406 -0.14215338 -0.2262721  -0.28678292 -0.04420045\n",
      " -0.16938041  0.13167404 -0.48115754  0.311955    0.03492964 -0.24378057\n",
      " -0.01857186  0.14245132  0.34674478  0.1304446   0.10107895 -0.01402427\n",
      " -0.42716584  0.0533957  -0.02644003  0.28163916 -0.30404797  0.03775847\n",
      "  0.23602998  0.4180529  -0.3853576  -0.04231958  0.13988984  0.21104898\n",
      " -0.15275496  0.31396255  0.14852124  0.07577474  0.16992833  0.36329278\n",
      "  0.11267725 -0.00078703  0.4309313   0.57610226 -0.1591827   0.5075882\n",
      "  0.12370792  0.1538571  -0.27071866  0.27979475 -0.2499536   0.25699\n",
      " -0.3561052   0.3070561  -0.58585924 -0.16401817 -0.0930964   0.34375778\n",
      "  0.34197727 -0.06881753  0.19097868  0.04069733 -0.15631074  0.23421101\n",
      "  0.18670802 -0.33176956 -0.3392932  -0.3425407   0.22726405  0.23575358\n",
      " -0.02917852  0.1869196  -0.01856603  0.5397542   0.00217351 -0.3456004\n",
      "  0.29737282 -0.14333302 -0.32234618 -0.3145977  -0.08286525  0.251937\n",
      " -0.13075042  0.20348373  0.05377951 -0.10597292  0.15767308  0.38776731\n",
      " -0.18100676  0.3943103   0.05694352 -0.47914526  0.11808696 -0.11467209\n",
      " -0.07804754 -0.01583259 -0.2897415   0.34862342 -0.0286342   0.12550345\n",
      " -0.16165772  0.19832799 -0.26378798 -0.12781842]\n",
      "\n",
      "Intrus from list : ['edmond','villefort','danglars','fernand'] \n",
      "edmond\n",
      " \n",
      "Most similar to edmond :\n",
      "[('dantes', 0.6650345325469971), ('mercedes', 0.5454688668251038), ('vengez', 0.49938005208969116), ('lamentable', 0.45577767491340637), ('hair', 0.4553127884864807), ('jacopo', 0.43795472383499146), ('brisait', 0.42358291149139404), ('venge', 0.41955363750457764), ('faria', 0.41137710213661194), ('tendresse', 0.4108629822731018)]\n",
      " \n",
      "Pos - Neg : maximilien - edmond + mercedes :\n",
      "[('valentine', 0.5495901703834534)]\n"
     ]
    }
   ],
   "source": [
    "# Print Raw word vector for a word\n",
    "iWord = \"edmond\"\n",
    "print(\"Word vector for word \"+ iWord + \" :\")\n",
    "print(model.wv[iWord])\n",
    "print(\"\")\n",
    "\n",
    "# Doesn't match\n",
    "print(\"Intrus from list : ['edmond','villefort','danglars','fernand'] \")\n",
    "print(model.wv.doesnt_match(['edmond','villefort','danglars','fernand']))\n",
    "print(\" \")\n",
    "\n",
    "# Most similar\n",
    "print(\"Most similar to \" + iWord + \" :\")\n",
    "print(model.wv.most_similar(iWord))\n",
    "print(\" \")\n",
    "\n",
    "# Positive & negative\n",
    "print(\"Pos - Neg : maximilien - edmond + mercedes :\")\n",
    "print(model.wv.most_similar(positive=[\"maximilien\",\"mercedes\"],negative=[\"edmond\"],topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of use with FT utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T13:58:29.811631Z",
     "start_time": "2018-06-12T13:58:29.802991Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normed word vector for word edmond :\n",
      "[ 0.08938088  0.29513335 -0.0536496  -0.08539654 -0.10823371 -0.01668153\n",
      " -0.06392525  0.04969463 -0.18159194  0.11773382  0.01318267 -0.09200435\n",
      " -0.00700914  0.05376204  0.13086371  0.04923063  0.03814784 -0.00529285\n",
      " -0.16121513  0.02015188 -0.00997864  0.10629243 -0.11474966  0.01425029\n",
      "  0.08907923  0.15777585 -0.14543642 -0.01597168  0.05279532  0.07965124\n",
      " -0.0576507   0.11849149  0.05605287  0.02859787  0.06413204  0.13710903\n",
      "  0.04252512 -0.00029703  0.16263624  0.21742469 -0.06007657  0.19156705\n",
      "  0.04668816  0.05806665 -0.10217096  0.10559633 -0.0943341   0.09698967\n",
      " -0.13439639  0.11588494 -0.22110704 -0.06190151 -0.03513518  0.1297364\n",
      "  0.12906441 -0.02597218  0.07207658  0.01535943 -0.05899268  0.08839274\n",
      "  0.0704648  -0.12521195 -0.12805143 -0.12927707  0.08577091  0.08897492\n",
      " -0.01101216  0.07054466 -0.00700694  0.20370671  0.0008203  -0.13043182\n",
      "  0.11223041 -0.0540948  -0.12165552 -0.11873119 -0.03127388  0.09508264\n",
      " -0.04934605  0.07679607  0.02029673 -0.03999486  0.05950683  0.14634587\n",
      " -0.06831311  0.14881524  0.02149085 -0.18083249  0.04456678 -0.04327798\n",
      " -0.02945564 -0.00597532 -0.10935029  0.13157272 -0.01080673  0.04736581\n",
      " -0.06101066  0.07485025 -0.09955528 -0.04823949]\n",
      "\n",
      "Normed word vector for sentence 'edmond dantes est monte-cristo' :\n",
      "[ 0.05680716  0.18767098 -0.08770168  0.01488744 -0.10063272 -0.06730412\n",
      " -0.111121   -0.03496699 -0.13621464  0.06572487  0.04880607 -0.01799668\n",
      "  0.00971182  0.02152672  0.07373299  0.08259393  0.0061777  -0.10657014\n",
      " -0.01908055 -0.03797291  0.02694565  0.04663585 -0.12099987  0.09073848\n",
      "  0.07594606  0.0361585  -0.02167     0.01496437  0.02256445 -0.00769839\n",
      " -0.11743738  0.10250628  0.04272459 -0.02918901  0.13667864  0.06119869\n",
      "  0.01390873  0.02663985  0.1408828   0.12716259 -0.01942609  0.08036254\n",
      "  0.04551614 -0.02695585 -0.10560521  0.02205866 -0.07231951  0.01308587\n",
      " -0.14831613  0.08823275 -0.13482681 -0.02237589 -0.06351845  0.0320711\n",
      "  0.07749527  0.00223716  0.04292387  0.07483455  0.02423465  0.03354798\n",
      " -0.00142754 -0.02279813 -0.0778297  -0.10302922 -0.0041321   0.04593146\n",
      "  0.00203713 -0.0131114   0.01686046  0.096861    0.00506261 -0.13998216\n",
      "  0.06191384 -0.06595194 -0.06805254 -0.06318603  0.03835414  0.1246566\n",
      "  0.0505479  -0.03547636  0.01317459 -0.03796855  0.05981745  0.01324269\n",
      " -0.02760504  0.140785   -0.0135086  -0.07233651  0.08704273 -0.02735206\n",
      " -0.05497894 -0.02439909 -0.07039922  0.01307827 -0.01055229 -0.002487\n",
      " -0.10793003  0.10813344 -0.07541188  0.00243257]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print normed word vector for a word\n",
    "iWord = \"edmond\"\n",
    "print(\"Normed word vector for word \"+ iWord + \" :\")\n",
    "print(ft.word_to_vec(iWord, model, iNormed=True))\n",
    "print(\"\")\n",
    "\n",
    "# Use ft utils to get vectors for a word list :\n",
    "iSentence = \"edmond dantes est monte-cristo\"\n",
    "print(\"Normed word vector for sentence '\"+ iSentence + \"' :\")\n",
    "print(ft.wordlist_to_vec(iSentence.split(\" \"), model, iNormed=True))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Matcher - may take some time to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ovnlp.txtMatcher import textMatcher as tm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load data\n",
    "df1 = pd.read_json(\"ovnlp/train.json\")\n",
    "df1.loc[:,\"ingredients\"] = df1.loc[:,\"ingredients\"].map(lambda x: \" \".join(y for y in x ))\n",
    "\n",
    "\n",
    "# Test on same DF matches recipes that are nealry the same\n",
    "txtMatcher = tm.TextMatcher(input_dfs=(df1, df1), \n",
    "                            text_cols = (u'ingredients', u'ingredients'), \n",
    "                            id_cols = ('id', 'id'))\n",
    "res_df = txtMatcher.get_results(threshold=0.9)\n",
    "\n",
    "\n",
    "# Test on same DF with group col : only matches recipes that are in the same cuisine category\n",
    "txtMatcher2 = tm.TextMatcher(input_dfs=(df1, df1), \n",
    "                             text_cols = (u'ingredients', u'ingredients'), \n",
    "                             id_cols = ('id', 'id'), \n",
    "                             group_cols=('cuisine', 'cuisine'))\n",
    "res_df2 = txtMatcher2.get_results(threshold=0.9)\n",
    "\n",
    "# Add groups : disjoint set of match :\n",
    "res_df3 = txtMatcher.get_results(threshold=0.9 , add_groups=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matched recipes :\n",
      "3596\n",
      "Number of matched recipes - only same cuisine category :\n",
      "2778\n",
      "Disjoint sets of recipe Ids :\n",
      "    groupId                                              group\n",
      "0      8256                                      (8256, 40523)\n",
      "1     34419                                     (34419, 44607)\n",
      "2     10276                 (10276, 37038, 13296, 13746, 2298)\n",
      "5     43970                                     (43970, 10332)\n",
      "6     41833                                     (41833, 23971)\n",
      "7     28232                                     (28232, 18031)\n",
      "8     29801                                     (29801, 32494)\n",
      "9     15273                                     (15273, 25599)\n",
      "11    40403                                     (40403, 15446)\n",
      "12    28496                                      (28496, 7666)\n",
      "13     9291                                (8250, 9291, 40111)\n",
      "15    11108                                     (11108, 17941)\n",
      "16    20792                                     (20792, 33853)\n",
      "17    18662  (43712, 27330, 32707, 23234, 14663, 33215, 474...\n",
      "22    48080  (29827, 5513, 17483, 24685, 23055, 48080, 1204...\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of matched recipes :\")\n",
    "print(res_df.shape[0])\n",
    "print(\"Number of matched recipes - only same cuisine category :\")\n",
    "print(res_df2.shape[0])\n",
    "print(\"Disjoint sets of recipe Ids :\")\n",
    "print(res_df3.loc[:,[\"groupId\",\"group\"]].drop_duplicates().head(15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
